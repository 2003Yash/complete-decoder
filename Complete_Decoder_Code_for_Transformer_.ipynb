{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoStwIe8Jm8YF/zkkjwxD/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/complete-decoder/blob/main/Complete_Decoder_Code_for_Transformer_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None): #used by multi head attention for normalizing q,k vector variance using softmax\n",
        "    d_k = tf.shape(q)[-1]\n",
        "    scaled = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) / math.sqrt(d_k)\n",
        "    print(f\"scaled.shape : {scaled.shape}\")\n",
        "    if mask is not None: #add mask on top of attention vector\n",
        "        print(f\"-- ADDING MASK of shape {mask.shape} --\")\n",
        "        scaled += mask #here mask is of different shape\n",
        "        # we use broadcasting i.e since scaled is 30 x 8 x 200 x 200 and mask is 200 x 200 so we put that 200 x 200 on over all those 30 x 8 vectors and derive an output\n",
        "    attention = tf.nn.softmax(scaled, axis=-1) # 30 x 8 x 200 x 200\n",
        "    values = tf.matmul(attention, v) # 30 x 8 x 200 x 64\n",
        "    return values, attention\n",
        "\n",
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.hidden = hidden\n",
        "        self.drop_prob = drop_prob\n",
        "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
        "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
        "        self.relu = tf.keras.layers.ReLU()\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        #  x: [batch_size, seq_len, d_model]\n",
        "        x = self.linear1(x)  # [batch_size, seq_len, hidden]\n",
        "        print(f\"x after first linear layer: {x.shape}\")\n",
        "        x = self.relu(x)     # [batch_size, seq_len, hidden]\n",
        "        print(f\"x after relu layer: {x.shape}\")\n",
        "        x = self.dropout(x, training=training)  # [batch_size, seq_len, hidden]\n",
        "        print(f\"x after dropout layer: {x.shape}\")\n",
        "        x = self.linear2(x)  # [batch_size, seq_len, d_model]\n",
        "        print(f\"x after 2nd linear layer: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "  # same as encoder layer normalization for the end of attenion vecotr  to norma and add\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = self.add_weight(shape=parameters_shape,\n",
        "                                     initializer='ones',\n",
        "                                     trainable=True,\n",
        "                                     name='gamma')\n",
        "        self.beta = self.add_weight(shape=parameters_shape,\n",
        "                                    initializer='zeros',\n",
        "                                    trainable=True,\n",
        "                                    name='beta')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # inputs : [batch_size, seq_len, d_model]\n",
        "        mean = tf.reduce_mean(inputs, axis=-1, keepdims=True)  # [batch_size, seq_len, 1]\n",
        "        var = tf.reduce_mean((inputs - mean) ** 2, axis=-1, keepdims=True)  # [batch_size, seq_len, 1]\n",
        "        std = tf.sqrt(var + self.eps)  # [batch_size, seq_len, 1]\n",
        "        y = (inputs - mean) / std  # [batch_size, seq_len, d_model]\n",
        "        out = self.gamma * y + self.beta  # [batch_size, seq_len, d_model]\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  #same concept from encoder\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model)\n",
        "        self.linear_layer = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x, mask=None, training=False):\n",
        "        batch_size, sequence_length, d_model = tf.shape(x)\n",
        "        print(f\"x.shape: {x.shape}\")\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.shape: {qkv.shape}\")\n",
        "        qkv = tf.reshape(qkv, (batch_size, sequence_length, self.num_heads, 3 * self.head_dim))\n",
        "        print(f\"qkv after reshape .shape: {qkv.shape}\")\n",
        "        qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "        print(f\"qkv after permutation: {qkv.shape}\")\n",
        "        q, k, v = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        print(f\"q: {q.shape}, k:{k.shape}, v:{v.shape}\")\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) # 30 x 8 x 200 x 64\n",
        "        print(f\"values: {values.shape}, attention:{attention.shape}\")\n",
        "        values = tf.reshape(values, (batch_size, sequence_length, self.num_heads * self.head_dim)) # 30 x 200 x 512\n",
        "        print(f\"values after reshaping: {values.shape}\")\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out after passing through linear layer: {out.shape}\")\n",
        "        return out # 30 x 200 x 512 same as the input dimensions\n",
        "\n",
        "class MultiHeadCrossAttention(tf.keras.layers.Layer):\n",
        " # very similar to working of multihead attention and self attention but only difference is here we are taking the q of english and k,v from telugu and translate the word\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadCrossAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = tf.keras.layers.Dense(2 * d_model)  # 1024\n",
        "        self.q_layer = tf.keras.layers.Dense(d_model)\n",
        "        self.linear_layer = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x, y, mask=None, training=False):\n",
        "        batch_size, sequence_length, d_model = tf.shape(x)\n",
        "        print(f\"x.shape: {x.shape}\")\n",
        "        kv = self.kv_layer(x)  # [batch_size, sequence_length, 2 * d_model]\n",
        "        print(f\"kv.shape: {kv.shape}\")\n",
        "        q = self.q_layer(y)  # [batch_size, sequence_length, d_model]\n",
        "        print(f\"q.shape: {q.shape}\")\n",
        "\n",
        "        kv = tf.reshape(kv, [batch_size, sequence_length, self.num_heads, 2 * self.head_dim])  # [batch_size, sequence_length, num_heads, 2 * head_dim]\n",
        "        q = tf.reshape(q, [batch_size, sequence_length, self.num_heads, self.head_dim])  # [batch_size, sequence_length, num_heads, head_dim]\n",
        "\n",
        "        kv = tf.transpose(kv, perm=[0, 2, 1, 3])  # [batch_size, num_heads, sequence_length, 2 * head_dim]\n",
        "        q = tf.transpose(q, perm=[0, 2, 1, 3])  # [batch_size, num_heads, sequence_length, head_dim]\n",
        "\n",
        "        k, v = tf.split(kv, num_or_size_splits=2, axis=-1)  # K: [batch_size, num_heads, sequence_length, head_dim], V: [batch_size, num_heads, sequence_length, head_dim]\n",
        "\n",
        "        values, attention = self.scaled_dot_product(q, k, v, mask)  # [batch_size, num_heads, sequence_length, head_dim]\n",
        "        print(f\"values: {values.shape}, attention: {attention.shape}\")\n",
        "\n",
        "        values = tf.reshape(values, [batch_size, sequence_length, d_model])  # [batch_size, sequence_length, d_model]\n",
        "        out = self.linear_layer(values)  # [batch_size, sequence_length, d_model]\n",
        "        print(f\"out after passing through linear layer: {out.shape}\")\n",
        "        return out  # [batch_size, sequence_length, d_model]\n",
        "\n",
        "    def scaled_dot_product(self, q, k, v, mask=None):\n",
        "        d_k = tf.shape(q)[-1]\n",
        "        scaled = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) / tf.sqrt(tf.cast(d_k, tf.float32))\n",
        "        if mask is not None:\n",
        "            scaled += mask\n",
        "        attention = tf.nn.softmax(scaled, axis=-1)\n",
        "        values = tf.matmul(attention, v)\n",
        "        return values, attention\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  #creates one complete decoder layer\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, y, decoder_mask, training=False):\n",
        "        _y = y # [batch_size, seq_len, d_model]\n",
        "        print(\"MASKED SELF ATTENTION\")\n",
        "        y = self.self_attention(y, mask=decoder_mask) # [batch_size, seq_len, d_model]\n",
        "        print(\"DROP OUT 1\")\n",
        "        y = self.dropout1(y, training=training) # [batch_size, seq_len, d_model]\n",
        "        print(\"ADD + LAYER NORMALIZATION 1\")\n",
        "        y = self.norm1(y + _y) # [batch_size, seq_len, d_model]\n",
        "\n",
        "        _y = y # [batch_size, seq_len, d_model]\n",
        "        print(\"CROSS ATTENTION\")\n",
        "        y = self.encoder_decoder_attention(x, y, mask=None) # [batch_size, seq_len, d_model]\n",
        "        print(\"DROP OUT 2\")\n",
        "        y = self.dropout2(y, training=training) # [batch_size, seq_len, d_model]\n",
        "        print(\"ADD + LAYER NORMALIZATION 2\")\n",
        "        y = self.norm2(y + _y) # [batch_size, seq_len, d_model]\n",
        "\n",
        "        _y = y # [batch_size, seq_len, d_model]\n",
        "        print(\"FEED FORWARD 1\")\n",
        "        y = self.ffn(y) # [batch_size, seq_len, d_model]\n",
        "        print(\"DROP OUT 3\")\n",
        "        y = self.dropout3(y, training=training) # [batch_size, seq_len, d_model]\n",
        "        print(\"ADD + LAYER NORMALIZATION 3\")\n",
        "        y = self.norm3(y + _y) # [batch_size, seq_len, d_model]\n",
        "        return y # [batch_size, seq_len, d_model]\n",
        "\n",
        "class SequentialDecoder(tf.keras.layers.Layer):\n",
        "  #create a single decoder block using architecture params\n",
        "    def __init__(self, *layers):\n",
        "        super(SequentialDecoder, self).__init__()\n",
        "        self.layers = layers\n",
        "\n",
        "    def call(self, x, y, mask, training=False):\n",
        "        for layer in self.layers:\n",
        "            y = layer(x, y, mask, training=training) # x value is value from encoder\n",
        "        return y\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer): #calling sequential decoder with architecture params\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = SequentialDecoder(*[\n",
        "            DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "            for _ in range(num_layers) #creating 5 decoders using same params\n",
        "        ])\n",
        "\n",
        "    def call(self, x, y, mask, training=False):\n",
        "      # x: 30 x 200 x 512\n",
        "      # y: 30 x 200 x 512\n",
        "      # mask: 200 x 200\n",
        "        y = self.layers(x, y, mask, training=training)\n",
        "        return y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMKubvz9wtsX",
        "outputId": "601b529b-9df3-4a11-b7a6-e3e24d07ffb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "x.shape: (30, 200, 512)\n",
            "qkv.shape: (30, 200, 1536)\n",
            "qkv after reshape .shape: (30, 200, 8, 192)\n",
            "qkv after permutation: (30, 8, 200, 192)\n",
            "q: (30, 8, 200, 64), k:(30, 8, 200, 64), v:(30, 8, 200, 64)\n",
            "scaled.shape : (30, 8, 200, 200)\n",
            "-- ADDING MASK of shape (200, 200) --\n",
            "values: (30, 8, 200, 64), attention:(30, 8, 200, 200)\n",
            "values after reshaping: (30, 200, 512)\n",
            "out after passing through linear layer: (30, 200, 512)\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "CROSS ATTENTION\n",
            "x.shape: (30, 200, 512)\n",
            "kv.shape: (30, 200, 1024)\n",
            "q.shape: (30, 200, 512)\n",
            "values: (30, 8, 200, 64), attention: (30, 8, 200, 200)\n",
            "out after passing through linear layer: (30, 200, 512)\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after relu layer: (30, 200, 2048)\n",
            "x after dropout layer: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after relu layer: (30, 200, 2048)\n",
            "x after dropout layer: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "x.shape: (30, 200, 512)\n",
            "qkv.shape: (30, 200, 1536)\n",
            "qkv after reshape .shape: (30, 200, 8, 192)\n",
            "qkv after permutation: (30, 8, 200, 192)\n",
            "q: (30, 8, 200, 64), k:(30, 8, 200, 64), v:(30, 8, 200, 64)\n",
            "scaled.shape : (30, 8, 200, 200)\n",
            "-- ADDING MASK of shape (200, 200) --\n",
            "values: (30, 8, 200, 64), attention:(30, 8, 200, 200)\n",
            "values after reshaping: (30, 200, 512)\n",
            "out after passing through linear layer: (30, 200, 512)\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "CROSS ATTENTION\n",
            "x.shape: (30, 200, 512)\n",
            "kv.shape: (30, 200, 1024)\n",
            "q.shape: (30, 200, 512)\n",
            "values: (30, 8, 200, 64), attention: (30, 8, 200, 200)\n",
            "out after passing through linear layer: (30, 200, 512)\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after relu layer: (30, 200, 2048)\n",
            "x after dropout layer: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after relu layer: (30, 200, 2048)\n",
            "x after dropout layer: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "x.shape: (30, 200, 512)\n",
            "qkv.shape: (30, 200, 1536)\n",
            "qkv after reshape .shape: (30, 200, 8, 192)\n",
            "qkv after permutation: (30, 8, 200, 192)\n",
            "q: (30, 8, 200, 64), k:(30, 8, 200, 64), v:(30, 8, 200, 64)\n",
            "scaled.shape : (30, 8, 200, 200)\n",
            "-- ADDING MASK of shape (200, 200) --\n",
            "values: (30, 8, 200, 64), attention:(30, 8, 200, 200)\n",
            "values after reshaping: (30, 200, 512)\n",
            "out after passing through linear layer: (30, 200, 512)\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "CROSS ATTENTION\n",
            "x.shape: (30, 200, 512)\n",
            "kv.shape: (30, 200, 1024)\n",
            "q.shape: (30, 200, 512)\n",
            "values: (30, 8, 200, 64), attention: (30, 8, 200, 200)\n",
            "out after passing through linear layer: (30, 200, 512)\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after relu layer: (30, 200, 2048)\n",
            "x after dropout layer: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after relu layer: (30, 200, 2048)\n",
            "x after dropout layer: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "x.shape: (30, 200, 512)\n",
            "qkv.shape: (30, 200, 1536)\n",
            "qkv after reshape .shape: (30, 200, 8, 192)\n",
            "qkv after permutation: (30, 8, 200, 192)\n",
            "q: (30, 8, 200, 64), k:(30, 8, 200, 64), v:(30, 8, 200, 64)\n",
            "scaled.shape : (30, 8, 200, 200)\n",
            "-- ADDING MASK of shape (200, 200) --\n",
            "values: (30, 8, 200, 64), attention:(30, 8, 200, 200)\n",
            "values after reshaping: (30, 200, 512)\n",
            "out after passing through linear layer: (30, 200, 512)\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "CROSS ATTENTION\n",
            "x.shape: (30, 200, 512)\n",
            "kv.shape: (30, 200, 1024)\n",
            "q.shape: (30, 200, 512)\n",
            "values: (30, 8, 200, 64), attention: (30, 8, 200, 200)\n",
            "out after passing through linear layer: (30, 200, 512)\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after relu layer: (30, 200, 2048)\n",
            "x after dropout layer: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after relu layer: (30, 200, 2048)\n",
            "x after dropout layer: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "MASKED SELF ATTENTION\n",
            "MASKED SELF ATTENTION\n",
            "x.shape: (30, 200, 512)\n",
            "qkv.shape: (30, 200, 1536)\n",
            "qkv after reshape .shape: (30, 200, 8, 192)\n",
            "qkv after permutation: (30, 8, 200, 192)\n",
            "q: (30, 8, 200, 64), k:(30, 8, 200, 64), v:(30, 8, 200, 64)\n",
            "scaled.shape : (30, 8, 200, 200)\n",
            "-- ADDING MASK of shape (200, 200) --\n",
            "values: (30, 8, 200, 64), attention:(30, 8, 200, 200)\n",
            "values after reshaping: (30, 200, 512)\n",
            "out after passing through linear layer: (30, 200, 512)\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "CROSS ATTENTION\n",
            "x.shape: (30, 200, 512)\n",
            "kv.shape: (30, 200, 1024)\n",
            "q.shape: (30, 200, 512)\n",
            "values: (30, 8, 200, 64), attention: (30, 8, 200, 200)\n",
            "out after passing through linear layer: (30, 200, 512)\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after relu layer: (30, 200, 2048)\n",
            "x after dropout layer: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after relu layer: (30, 200, 2048)\n",
            "x after dropout layer: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1 #we use dropout to enable model to learn patterns in multiple paths\n",
        "batch_size = 30 #no.of sentences to we take for batch optimization\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048 #no.of neural network nodes -> 2048 is just an hyper param in the research paper \"attenion is all you need\"\n",
        "num_layers = 5 #no.of decoders we use to pass the data through"
      ],
      "metadata": {
        "id": "uiuduSYH1szQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Inputs\n",
        "x = tf.random.normal((batch_size, max_sequence_length, d_model))  # random English sentence positional encoded\n",
        "y = tf.random.normal((batch_size, max_sequence_length, d_model))  # random telugu sentence positional encoded\n",
        "\n",
        "# Mask\n",
        "mask = tf.fill([max_sequence_length, max_sequence_length], float('-inf'))\n",
        "mask = tf.linalg.band_part(mask, 0, -1) #creating mask using tensorflow\n",
        "\n",
        "# Decoder\n",
        "decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
        "out = decoder(x, y, mask, training=True) # creating decoder with it's architecture param values"
      ],
      "metadata": {
        "id": "aQ8ZXAu91vmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask #just looking at mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKV8UellwYM1",
        "outputId": "41c6ded2-c47b-42ac-a867-0a307a50df93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(200, 200), dtype=float32, numpy=\n",
              "array([[-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
              "       [  0., -inf, -inf, ..., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, ..., -inf, -inf, -inf],\n",
              "       ...,\n",
              "       [  0.,   0.,   0., ..., -inf, -inf, -inf],\n",
              "       [  0.,   0.,   0., ...,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., ...,   0.,   0., -inf]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}